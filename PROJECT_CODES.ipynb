{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPri9n0cnNL1OuxNTEAIboN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51383/PDS-PROJECT.1383/blob/main/PROJECT_CODES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score\n",
        "\n",
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# ---------- Settings (adjust if needed) ----------\n",
        "TEST_SIZE = 0.30\n",
        "RANDOM_STATE = 42\n",
        "SAMPLE_MAX = 10000   # set None to use full dataset (may be slow)\n",
        "# -------------------------------------------------\n",
        "\n",
        "# 1) Auto-detect CSV in /content/\n",
        "DATA_PATH = None\n",
        "for fname in os.listdir(\"/content\"):\n",
        "    if fname.lower().endswith(\".csv\"):\n",
        "        DATA_PATH = os.path.join(\"/content\", fname)\n",
        "        break\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError(\"No CSV file found in /content/. Please upload your Data.csv file to Colab's /content/ folder.\")\n",
        "\n",
        "print(\"Using dataset:\", DATA_PATH)\n",
        "\n",
        "# 2) Load dataset\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Loaded Data.csv — shape:\", df.shape)\n",
        "print(\"First 20 columns:\", list(df.columns)[:20], \" ...\")\n",
        "\n",
        "# 3) Identify or create target column\n",
        "possible_target_names = ['target','label','class','y','emission_class','high_emission','High_CO2','high_co2']\n",
        "target_col = None\n",
        "for t in possible_target_names:\n",
        "    if t in df.columns:\n",
        "        target_col = t\n",
        "        break\n",
        "\n",
        "co2_candidates = [c for c in df.columns if any(k in c.lower() for k in ['co2','carbon','emiss','ghg'])]\n",
        "\n",
        "if target_col is None:\n",
        "    if co2_candidates:\n",
        "        co2_col = co2_candidates[0]\n",
        "        print(f\"No explicit target found. Creating binary target from '{co2_col}' using median split.\")\n",
        "        df['high_emission'] = (df[co2_col] > df[co2_col].median()).astype(int)\n",
        "        target_col = 'high_emission'\n",
        "    else:\n",
        "        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if not num_cols:\n",
        "            raise ValueError(\"No numeric columns found to create a target. Provide a labeled dataset or numeric emissions column.\")\n",
        "        fallback = num_cols[-1]\n",
        "        print(f\"No CO2-like column found. Creating binary target from '{fallback}' using median split.\")\n",
        "        df['high_emission'] = (df[fallback] > df[fallback].median()).astype(int)\n",
        "        target_col = 'high_emission'\n",
        "else:\n",
        "    print(f\"Using existing target column: '{target_col}'\")\n",
        "\n",
        "print(\"\\nTarget distribution (value_counts):\")\n",
        "print(df[target_col].value_counts())\n",
        "\n",
        "# 4) Use numeric features only (exclude the target)\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_features = [c for c in numeric_features if c != target_col]\n",
        "\n",
        "if len(numeric_features) == 0:\n",
        "    raise ValueError(\"No numeric features found for modeling. Consider encoding categorical features.\")\n",
        "\n",
        "# 5) Optionally perform stratified sampling to limit runtime\n",
        "if SAMPLE_MAX is not None and len(df) > SAMPLE_MAX:\n",
        "    print(f\"\\nDataset has {len(df)} rows; attempting stratified sample of {SAMPLE_MAX} rows (by '{target_col}').\")\n",
        "    try:\n",
        "        # train_test_split accepts an int for train_size and supports stratify\n",
        "        sampled_df, _ = train_test_split(df, train_size=SAMPLE_MAX, stratify=df[target_col], random_state=RANDOM_STATE)\n",
        "        X = sampled_df[numeric_features].copy()\n",
        "        y = sampled_df[target_col].copy()\n",
        "        print(\"Stratified sampling successful. Sampled shape:\", X.shape)\n",
        "    except Exception as e:\n",
        "        # fallback: random sample (no stratify)\n",
        "        print(\"Stratified sampling failed (likely due to class imbalance). Falling back to random sample. Error:\", e)\n",
        "        sampled_df = df.sample(n=SAMPLE_MAX, random_state=RANDOM_STATE)\n",
        "        X = sampled_df[numeric_features].copy()\n",
        "        y = sampled_df[target_col].copy()\n",
        "        print(\"Random sampling done. Sampled shape:\", X.shape)\n",
        "else:\n",
        "    X = df[numeric_features].copy()\n",
        "    y = df[target_col].copy()\n",
        "    print(f\"\\nUsing full dataset for modeling. Rows: {len(X)}\")\n",
        "\n",
        "# 6) Train-test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n",
        "print(\"\\nTrain/Test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# 7) Preprocessing (numeric): impute + scale\n",
        "num_pipe = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
        "X_train_proc = num_pipe.fit_transform(X_train)\n",
        "X_test_proc = num_pipe.transform(X_test)\n",
        "\n",
        "# 8) Classifiers to run\n",
        "classifiers = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1000, solver='liblinear', random_state=RANDOM_STATE),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5, n_jobs=-1),\n",
        "    'SVM_rbf': SVC(kernel='rbf', random_state=RANDOM_STATE),\n",
        "    'DecisionTree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "    'ExtraTrees': ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE),\n",
        "    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'AdaBoost': AdaBoostClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "    'GaussianNB': GaussianNB(),\n",
        "    'LDA': LinearDiscriminantAnalysis(),\n",
        "    'QDA': QuadraticDiscriminantAnalysis(),\n",
        "    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "# 9) Train & evaluate\n",
        "results = []\n",
        "for name, clf in classifiers.items():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model:\", name)\n",
        "    try:\n",
        "        clf.fit(X_train_proc, y_train)\n",
        "        y_pred = clf.predict(X_test_proc)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        bal = balanced_accuracy_score(y_test, y_pred)\n",
        "        print(\"Accuracy:\", round(acc,4), \" Balanced Acc:\", round(bal,4))\n",
        "        print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "        print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "        results.append({'model': name, 'accuracy': acc, 'balanced_accuracy': bal, 'note': ''})\n",
        "    except Exception as ex:\n",
        "        print(f\"Model {name} failed: {ex}\")\n",
        "        results.append({'model': name, 'accuracy': np.nan, 'balanced_accuracy': np.nan, 'note': str(ex)})\n",
        "\n",
        "# 10) Summary & save\n",
        "results_df = pd.DataFrame(results).sort_values(by='balanced_accuracy', ascending=False).reset_index(drop=True)\n",
        "print(\"\\nFinal summary (sorted by balanced accuracy):\")\n",
        "print(results_df)\n",
        "\n",
        "OUTPATH = \"/content/classification_results_summary.csv\"\n",
        "results_df.to_csv(OUTPATH, index=False)\n",
        "print(\"\\nSaved summary CSV to:\", OUTPATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbs_GVS34pUD",
        "outputId": "cd8575b3-24cc-4730-f03c-76175c9a19d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using dataset: /content/Data.csv\n",
            "Loaded Data.csv — shape: (43746, 80)\n",
            "First 20 columns: ['Description', 'Name', 'year', 'iso_code', 'population', 'gdp', 'cement_co2', 'cement_co2_per_capita', 'co2', 'co2_growth_abs', 'co2_growth_prct', 'co2_including_luc', 'co2_including_luc_growth_abs', 'co2_including_luc_growth_prct', 'co2_including_luc_per_capita', 'co2_including_luc_per_gdp', 'co2_including_luc_per_unit_energy', 'co2_per_capita', 'co2_per_gdp', 'co2_per_unit_energy']  ...\n",
            "No explicit target found. Creating binary target from 'cement_co2' using median split.\n",
            "\n",
            "Target distribution (value_counts):\n",
            "high_emission\n",
            "0    31942\n",
            "1    11804\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Dataset has 43746 rows; attempting stratified sample of 10000 rows (by 'high_emission').\n",
            "Stratified sampling successful. Sampled shape: (10000, 77)\n",
            "\n",
            "Train/Test shapes: (7000, 77) (3000, 77)\n",
            "\n",
            "============================================================\n",
            "Model: LogisticRegression\n",
            "Accuracy: 0.9713  Balanced Acc: 0.9507\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9663    0.9954    0.9807      2191\n",
            "           1     0.9865    0.9061    0.9446       809\n",
            "\n",
            "    accuracy                         0.9713      3000\n",
            "   macro avg     0.9764    0.9507    0.9626      3000\n",
            "weighted avg     0.9718    0.9713    0.9709      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2181   10]\n",
            " [  76  733]]\n",
            "\n",
            "============================================================\n",
            "Model: KNN\n",
            "Accuracy: 0.953  Balanced Acc: 0.9366\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9638    0.9722    0.9680      2191\n",
            "           1     0.9228    0.9011    0.9118       809\n",
            "\n",
            "    accuracy                         0.9530      3000\n",
            "   macro avg     0.9433    0.9366    0.9399      3000\n",
            "weighted avg     0.9527    0.9530    0.9528      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2130   61]\n",
            " [  80  729]]\n",
            "\n",
            "============================================================\n",
            "Model: SVM_rbf\n",
            "Accuracy: 0.9493  Balanced Acc: 0.915\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9438    0.9895    0.9661      2191\n",
            "           1     0.9673    0.8405    0.8995       809\n",
            "\n",
            "    accuracy                         0.9493      3000\n",
            "   macro avg     0.9556    0.9150    0.9328      3000\n",
            "weighted avg     0.9502    0.9493    0.9482      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2168   23]\n",
            " [ 129  680]]\n",
            "\n",
            "============================================================\n",
            "Model: DecisionTree\n",
            "Accuracy: 1.0  Balanced Acc: 1.0\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000      2191\n",
            "           1     1.0000    1.0000    1.0000       809\n",
            "\n",
            "    accuracy                         1.0000      3000\n",
            "   macro avg     1.0000    1.0000    1.0000      3000\n",
            "weighted avg     1.0000    1.0000    1.0000      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2191    0]\n",
            " [   0  809]]\n",
            "\n",
            "============================================================\n",
            "Model: RandomForest\n",
            "Accuracy: 1.0  Balanced Acc: 1.0\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000      2191\n",
            "           1     1.0000    1.0000    1.0000       809\n",
            "\n",
            "    accuracy                         1.0000      3000\n",
            "   macro avg     1.0000    1.0000    1.0000      3000\n",
            "weighted avg     1.0000    1.0000    1.0000      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2191    0]\n",
            " [   0  809]]\n",
            "\n",
            "============================================================\n",
            "Model: ExtraTrees\n",
            "Accuracy: 0.9907  Balanced Acc: 0.9893\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9950    0.9922    0.9936      2191\n",
            "           1     0.9791    0.9864    0.9828       809\n",
            "\n",
            "    accuracy                         0.9907      3000\n",
            "   macro avg     0.9871    0.9893    0.9882      3000\n",
            "weighted avg     0.9907    0.9907    0.9907      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2174   17]\n",
            " [  11  798]]\n",
            "\n",
            "============================================================\n",
            "Model: GradientBoosting\n",
            "Accuracy: 1.0  Balanced Acc: 1.0\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000      2191\n",
            "           1     1.0000    1.0000    1.0000       809\n",
            "\n",
            "    accuracy                         1.0000      3000\n",
            "   macro avg     1.0000    1.0000    1.0000      3000\n",
            "weighted avg     1.0000    1.0000    1.0000      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2191    0]\n",
            " [   0  809]]\n",
            "\n",
            "============================================================\n",
            "Model: AdaBoost\n",
            "Accuracy: 1.0  Balanced Acc: 1.0\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000      2191\n",
            "           1     1.0000    1.0000    1.0000       809\n",
            "\n",
            "    accuracy                         1.0000      3000\n",
            "   macro avg     1.0000    1.0000    1.0000      3000\n",
            "weighted avg     1.0000    1.0000    1.0000      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2191    0]\n",
            " [   0  809]]\n",
            "\n",
            "============================================================\n",
            "Model: GaussianNB\n",
            "Accuracy: 0.9223  Balanced Acc: 0.879\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9245    0.9731    0.9482      2191\n",
            "           1     0.9150    0.7849    0.8450       809\n",
            "\n",
            "    accuracy                         0.9223      3000\n",
            "   macro avg     0.9198    0.8790    0.8966      3000\n",
            "weighted avg     0.9220    0.9223    0.9204      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2132   59]\n",
            " [ 174  635]]\n",
            "\n",
            "============================================================\n",
            "Model: LDA\n",
            "Accuracy: 0.9217  Balanced Acc: 0.8735\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9197    0.9781    0.9480      2191\n",
            "           1     0.9284    0.7689    0.8411       809\n",
            "\n",
            "    accuracy                         0.9217      3000\n",
            "   macro avg     0.9241    0.8735    0.8946      3000\n",
            "weighted avg     0.9221    0.9217    0.9192      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2143   48]\n",
            " [ 187  622]]\n",
            "\n",
            "============================================================\n",
            "Model: QDA\n",
            "Accuracy: 0.9523  Balanced Acc: 0.9253\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9523    0.9840    0.9679      2191\n",
            "           1     0.9524    0.8665    0.9074       809\n",
            "\n",
            "    accuracy                         0.9523      3000\n",
            "   macro avg     0.9524    0.9253    0.9377      3000\n",
            "weighted avg     0.9523    0.9523    0.9516      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2156   35]\n",
            " [ 108  701]]\n",
            "\n",
            "============================================================\n",
            "Model: MLP\n",
            "Accuracy: 0.9867  Balanced Acc: 0.98\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9873    0.9945    0.9909      2191\n",
            "           1     0.9849    0.9654    0.9750       809\n",
            "\n",
            "    accuracy                         0.9867      3000\n",
            "   macro avg     0.9861    0.9800    0.9830      3000\n",
            "weighted avg     0.9867    0.9867    0.9866      3000\n",
            "\n",
            "Confusion matrix:\n",
            " [[2179   12]\n",
            " [  28  781]]\n",
            "\n",
            "Final summary (sorted by balanced accuracy):\n",
            "                 model  accuracy  balanced_accuracy note\n",
            "0         DecisionTree  1.000000           1.000000     \n",
            "1             AdaBoost  1.000000           1.000000     \n",
            "2     GradientBoosting  1.000000           1.000000     \n",
            "3         RandomForest  1.000000           1.000000     \n",
            "4           ExtraTrees  0.990667           0.989322     \n",
            "5                  MLP  0.986667           0.979956     \n",
            "6   LogisticRegression  0.971333           0.950746     \n",
            "7                  KNN  0.953000           0.936636     \n",
            "8                  QDA  0.952333           0.925264     \n",
            "9              SVM_rbf  0.949333           0.915023     \n",
            "10          GaussianNB  0.922333           0.878996     \n",
            "11                 LDA  0.921667           0.873471     \n",
            "\n",
            "Saved summary CSV to: /content/classification_results_summary.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# ---------------- Settings ----------------\n",
        "SAMPLE_MAX = 3000      # smaller sample for speed\n",
        "PCA_COMPONENTS = 10    # reduce to 10 dimensions\n",
        "RANDOM_STATE = 42\n",
        "OUT_DIR = \"/content\"\n",
        "# ------------------------------------------\n",
        "\n",
        "# 1) Load CSV from /content/\n",
        "DATA_PATH = None\n",
        "for fname in os.listdir(\"/content\"):\n",
        "    if fname.lower().endswith(\".csv\"):\n",
        "        DATA_PATH = os.path.join(\"/content\", fname)\n",
        "        break\n",
        "if DATA_PATH is None:\n",
        "    raise FileNotFoundError(\"Upload Data.csv to /content/ first.\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Loaded shape:\", df.shape)\n",
        "\n",
        "# 2) Numeric features only\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if not numeric_cols:\n",
        "    raise ValueError(\"No numeric columns found.\")\n",
        "X = df[numeric_cols].copy()\n",
        "\n",
        "# 3) Optional sampling\n",
        "if len(X) > SAMPLE_MAX:\n",
        "    X = X.sample(n=SAMPLE_MAX, random_state=RANDOM_STATE)\n",
        "print(\"Working data shape:\", X.shape)\n",
        "\n",
        "# 4) Scale + PCA\n",
        "X = X.fillna(X.median())\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "print(\"After PCA:\", X_reduced.shape)\n",
        "\n",
        "# 5) Define models (fast ones only)\n",
        "models = {\n",
        "    \"KMeans_3\": KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10),\n",
        "    \"Agglomerative_3\": AgglomerativeClustering(n_clusters=3, linkage=\"ward\"),\n",
        "    \"GMM_3\": GaussianMixture(n_components=3, random_state=RANDOM_STATE),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.7, min_samples=10),\n",
        "    \"Birch_3\": Birch(n_clusters=3)\n",
        "}\n",
        "\n",
        "# 6) Run models and compute metrics\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    print(\"\\nRunning:\", name)\n",
        "    if name == \"GMM_3\":\n",
        "        model.fit(X_reduced)\n",
        "        labels = model.predict(X_reduced)\n",
        "    else:\n",
        "        labels = model.fit_predict(X_reduced)\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    if len(unique_labels) > 1:\n",
        "        sil = silhouette_score(X_reduced, labels)\n",
        "        db = davies_bouldin_score(X_reduced, labels)\n",
        "        ch = calinski_harabasz_score(X_reduced, labels)\n",
        "    else:\n",
        "        sil = db = ch = np.nan\n",
        "\n",
        "    print(f\"Clusters: {len(unique_labels)} | Silhouette: {sil:.3f}\")\n",
        "    results.append({\n",
        "        \"model\": name,\n",
        "        \"n_clusters\": len(unique_labels),\n",
        "        \"silhouette\": sil,\n",
        "        \"davies_bouldin\": db,\n",
        "        \"calinski_harabasz\": ch\n",
        "    })\n",
        "\n",
        "# 7) Save summary\n",
        "results_df = pd.DataFrame(results)\n",
        "outpath = os.path.join(OUT_DIR, \"unsupervised_fast_summary.csv\")\n",
        "results_df.to_csv(outpath, index=False)\n",
        "print(\"\\nSaved summary to:\", outpath)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_sLm-sS4vBt",
        "outputId": "81445301-3676-4e60-f970-58b494dc938e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded shape: (43746, 80)\n",
            "Working data shape: (3000, 77)\n",
            "After PCA: (3000, 10)\n",
            "\n",
            "Running: KMeans_3\n",
            "Clusters: 3 | Silhouette: 0.820\n",
            "\n",
            "Running: Agglomerative_3\n",
            "Clusters: 3 | Silhouette: 0.819\n",
            "\n",
            "Running: GMM_3\n",
            "Clusters: 3 | Silhouette: 0.270\n",
            "\n",
            "Running: DBSCAN\n",
            "Clusters: 3 | Silhouette: 0.444\n",
            "\n",
            "Running: Birch_3\n",
            "Clusters: 3 | Silhouette: 0.783\n",
            "\n",
            "Saved summary to: /content/unsupervised_fast_summary.csv\n",
            "             model  n_clusters  silhouette  davies_bouldin  calinski_harabasz\n",
            "0         KMeans_3           3    0.820027        0.867712        1650.828469\n",
            "1  Agglomerative_3           3    0.819433        0.904204        1529.035491\n",
            "2            GMM_3           3    0.269946        1.701338         921.658256\n",
            "3           DBSCAN           3    0.444338        1.745766         114.941064\n",
            "4          Birch_3           3    0.783013        0.934652        1512.633558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "\n",
        "# -----------------------\n",
        "# User settings\n",
        "# -----------------------\n",
        "DATA_PATH = \"/content/Data.csv\"   # Change if needed\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.25\n",
        "VAL_SIZE = 0.1\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "PATIENCE = 3\n",
        "\n",
        "# -----------------------\n",
        "# 1) Load dataset\n",
        "# -----------------------\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    raise FileNotFoundError(f\"No file found at {DATA_PATH}\")\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# -----------------------\n",
        "# 2) Target creation\n",
        "# -----------------------\n",
        "possible_target_names = ['target','label','class','y','emission_class','high_emission']\n",
        "target_col = None\n",
        "for t in possible_target_names:\n",
        "    if t in df.columns:\n",
        "        target_col = t\n",
        "        break\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target_col is None:\n",
        "    if numeric_cols:\n",
        "        fallback = numeric_cols[-1]\n",
        "        print(\"No explicit target found. Creating binary target from:\", fallback)\n",
        "        df['high_emission'] = (df[fallback] > df[fallback].median()).astype(int)\n",
        "        target_col = 'high_emission'\n",
        "    else:\n",
        "        raise ValueError(\"No numeric target found.\")\n",
        "\n",
        "print(\"Target distribution:\\n\", df[target_col].value_counts())\n",
        "\n",
        "# -----------------------\n",
        "# 3) Feature selection\n",
        "# -----------------------\n",
        "drop_like = ['id','ID','index']\n",
        "cols = [c for c in df.columns if c not in drop_like + [target_col]]\n",
        "\n",
        "num_cols = [c for c in cols if np.issubdtype(df[c].dtype, np.number)]\n",
        "cat_cols = [c for c in cols if c not in num_cols]\n",
        "\n",
        "X_num = df[num_cols].copy() if num_cols else pd.DataFrame(index=df.index)\n",
        "X_cat = df[cat_cols].copy() if cat_cols else pd.DataFrame(index=df.index)\n",
        "y = df[target_col].astype(int)\n",
        "\n",
        "# -----------------------\n",
        "# 4) Preprocess\n",
        "# -----------------------\n",
        "# numeric\n",
        "if not X_num.empty:\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X_num_imp = pd.DataFrame(num_imputer.fit_transform(X_num), columns=num_cols)\n",
        "    scaler = StandardScaler()\n",
        "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num_imp), columns=num_cols)\n",
        "else:\n",
        "    X_num_scaled = pd.DataFrame(index=df.index)\n",
        "\n",
        "# categorical (only if dataset is reasonably sized)\n",
        "if not X_cat.empty and len(df) > 20:\n",
        "    X_cat_filled = X_cat.fillna(\"MISSING\").astype(str)\n",
        "    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "    X_cat_ohe = ohe.fit_transform(X_cat_filled)\n",
        "else:\n",
        "    X_cat_ohe = None\n",
        "\n",
        "if X_cat_ohe is not None:\n",
        "    X_full = np.hstack([X_num_scaled.values, X_cat_ohe])\n",
        "else:\n",
        "    X_full = X_num_scaled.values\n",
        "\n",
        "print(\"Final input shape:\", X_full.shape)\n",
        "\n",
        "# -----------------------\n",
        "# 5) Train/val/test split\n",
        "# -----------------------\n",
        "if len(df) < 30:\n",
        "    # tiny dataset: skip validation split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_full, y.values, test_size=0.4, random_state=RANDOM_STATE, stratify=y.values if len(set(y.values))>1 else None\n",
        "    )\n",
        "    X_val, y_val = X_test, y_test\n",
        "else:\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X_full, y.values, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y.values if len(set(y.values))>1 else None\n",
        "    )\n",
        "    val_fraction = VAL_SIZE / (1 - TEST_SIZE)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_full, y_train_full, test_size=val_fraction, random_state=RANDOM_STATE,\n",
        "        stratify=y_train_full if len(set(y_train_full))>1 else None\n",
        "    )\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Val shape:\", X_val.shape, \"Test shape:\", X_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# 6) Model: simple MLP\n",
        "# -----------------------\n",
        "def build_mlp(input_dim):\n",
        "    inputs = keras.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = keras.Model(inputs, outputs, name=\"mlp\")\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "mlp = build_mlp(X_train.shape[1])\n",
        "early = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True, verbose=0)\n",
        "\n",
        "print(\"\\nTraining MLP...\")\n",
        "mlp.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE, callbacks=[early], verbose=0)\n",
        "\n",
        "# -----------------------\n",
        "# 7) Evaluation\n",
        "# -----------------------\n",
        "y_pred_proba = mlp.predict(X_test).ravel()\n",
        "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n=== MLP Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "try:\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(\"AUC:\", auc)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "mlp.save(\"/content/mlp_model.keras\")\n",
        "print(\"Saved model to /content/mlp_model.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMBaGPQb44Ck",
        "outputId": "789af988-b254-4b8a-9563-1e7f8eb36650"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.19.0\n",
            "Dataset shape: (43746, 80)\n",
            "Columns: ['Description', 'Name', 'year', 'iso_code', 'population', 'gdp', 'cement_co2', 'cement_co2_per_capita', 'co2', 'co2_growth_abs', 'co2_growth_prct', 'co2_including_luc', 'co2_including_luc_growth_abs', 'co2_including_luc_growth_prct', 'co2_including_luc_per_capita', 'co2_including_luc_per_gdp', 'co2_including_luc_per_unit_energy', 'co2_per_capita', 'co2_per_gdp', 'co2_per_unit_energy', 'coal_co2', 'coal_co2_per_capita', 'consumption_co2', 'consumption_co2_per_capita', 'consumption_co2_per_gdp', 'cumulative_cement_co2', 'cumulative_co2', 'cumulative_co2_including_luc', 'cumulative_coal_co2', 'cumulative_flaring_co2', 'cumulative_gas_co2', 'cumulative_luc_co2', 'cumulative_oil_co2', 'cumulative_other_co2', 'energy_per_capita', 'energy_per_gdp', 'flaring_co2', 'flaring_co2_per_capita', 'gas_co2', 'gas_co2_per_capita', 'ghg_excluding_lucf_per_capita', 'ghg_per_capita', 'land_use_change_co2', 'land_use_change_co2_per_capita', 'methane', 'methane_per_capita', 'nitrous_oxide', 'nitrous_oxide_per_capita', 'oil_co2', 'oil_co2_per_capita', 'other_co2_per_capita', 'other_industry_co2', 'primary_energy_consumption', 'share_global_cement_co2', 'share_global_co2', 'share_global_co2_including_luc', 'share_global_coal_co2', 'share_global_cumulative_cement_co2', 'share_global_cumulative_co2', 'share_global_cumulative_co2_including_luc', 'share_global_cumulative_coal_co2', 'share_global_cumulative_flaring_co2', 'share_global_cumulative_gas_co2', 'share_global_cumulative_luc_co2', 'share_global_cumulative_oil_co2', 'share_global_cumulative_other_co2', 'share_global_flaring_co2', 'share_global_gas_co2', 'share_global_luc_co2', 'share_global_oil_co2', 'share_global_other_co2', 'share_of_temperature_change_from_ghg', 'temperature_change_from_ch4', 'temperature_change_from_co2', 'temperature_change_from_ghg', 'temperature_change_from_n2o', 'total_ghg', 'total_ghg_excluding_lucf', 'trade_co2', 'trade_co2_share']\n",
            "No explicit target found. Creating binary target from: trade_co2_share\n",
            "Target distribution:\n",
            " high_emission\n",
            "0    41479\n",
            "1     2267\n",
            "Name: count, dtype: int64\n",
            "Final input shape: (43746, 554)\n",
            "Train shape: (28434, 554) Val shape: (4375, 554) Test shape: (10937, 554)\n",
            "\n",
            "Training MLP...\n",
            "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "=== MLP Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9988    0.9992    0.9990     10370\n",
            "           1     0.9858    0.9788    0.9823       567\n",
            "\n",
            "    accuracy                         0.9982     10937\n",
            "   macro avg     0.9923    0.9890    0.9907     10937\n",
            "weighted avg     0.9982    0.9982    0.9982     10937\n",
            "\n",
            "AUC: 0.9998921730197847\n",
            "Saved model to /content/mlp_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"gymnasium[classic-control]\" \"stable-baselines3[extra]\" torch scikit-learn pandas numpy\n",
        "\n",
        "import os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# ------------------------\n",
        "# Settings\n",
        "# ------------------------\n",
        "DATA_PATH = \"/content/Data.csv\"\n",
        "RANDOM_STATE = 42\n",
        "TIMESTEPS = 1000      # short training\n",
        "MAX_EPISODE_STEPS = 8\n",
        "RF_TREES = 6\n",
        "REWARD_ALPHA = 1.0\n",
        "REWARD_BETA  = 1.0\n",
        "\n",
        "# ------------------------\n",
        "# Load dataset\n",
        "# ------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "co2_col = [c for c in numeric_cols if \"co2\" in c.lower()][0]\n",
        "gdp_col = [c for c in numeric_cols if \"gdp\" in c.lower()][0]\n",
        "state_cols = [c for c in numeric_cols if c not in (co2_col, gdp_col)]\n",
        "\n",
        "if not state_cols:\n",
        "    state_cols = [co2_col, gdp_col]\n",
        "\n",
        "print(\"Using CO2:\", co2_col, \"| GDP:\", gdp_col)\n",
        "\n",
        "# ------------------------\n",
        "# Synthetic deltas\n",
        "# ------------------------\n",
        "tmp = df[state_cols].fillna(0)\n",
        "tmp_norm = (tmp - tmp.mean()) / (tmp.std().replace(0,1))\n",
        "rng = np.random.RandomState(RANDOM_STATE)\n",
        "df['delta_co2'] = -0.01*tmp_norm.sum(axis=1) + rng.normal(scale=0.01, size=len(df))\n",
        "df['delta_gdp'] =  0.01*tmp_norm.sum(axis=1) + rng.normal(scale=0.01, size=len(df))\n",
        "\n",
        "# ------------------------\n",
        "# Train regressors\n",
        "# ------------------------\n",
        "X_state = df[state_cols].fillna(0).values\n",
        "scaler_state = StandardScaler().fit(X_state)\n",
        "X_state_scaled = scaler_state.transform(X_state)\n",
        "actions = rng.choice([-1,0,1], size=len(df))\n",
        "X_reg = np.hstack([X_state_scaled, actions.reshape(-1,1)])\n",
        "rf_co2 = RandomForestRegressor(n_estimators=RF_TREES, random_state=RANDOM_STATE).fit(X_reg, df['delta_co2'])\n",
        "rf_gdp = RandomForestRegressor(n_estimators=RF_TREES, random_state=RANDOM_STATE).fit(X_reg, df['delta_gdp'])\n",
        "\n",
        "# ------------------------\n",
        "# Environment\n",
        "# ------------------------\n",
        "class IndustrialEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # -1,0,+1\n",
        "        self.observation_space = spaces.Box(low=-5, high=5, shape=(len(state_cols),), dtype=np.float32)\n",
        "        self.max_steps = MAX_EPISODE_STEPS\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        idx = np.random.randint(0, len(df))\n",
        "        self.state_raw = df.loc[idx, state_cols].fillna(0).values.astype(float)\n",
        "        self.co2 = float(df.loc[idx, co2_col])\n",
        "        self.gdp = float(df.loc[idx, gdp_col])\n",
        "        self.steps = 0\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = scaler_state.transform(self.state_raw.reshape(1,-1))[0]\n",
        "        obs = np.nan_to_num(obs, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        return np.clip(obs, -5, 5).astype(np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        action_val = [-1,0,1][int(action)]\n",
        "        X_input = np.hstack([self._get_obs(), action_val]).reshape(1,-1)\n",
        "\n",
        "        delta_co2 = np.nan_to_num(rf_co2.predict(X_input)[0], nan=0.0)\n",
        "        delta_gdp = np.nan_to_num(rf_gdp.predict(X_input)[0], nan=0.0)\n",
        "\n",
        "        delta_co2 = float(np.clip(delta_co2, -0.5, 0.5))\n",
        "        delta_gdp = float(np.clip(delta_gdp, -0.5, 0.5))\n",
        "\n",
        "        prev_co2, prev_gdp = max(self.co2, 1e-6), max(self.gdp, 1e-6)\n",
        "        self.co2 = np.clip(self.co2 + delta_co2, 0, 1e6)\n",
        "        self.gdp = np.clip(self.gdp + delta_gdp, 0, 1e6)\n",
        "\n",
        "        reward = REWARD_ALPHA*(delta_gdp/(abs(prev_gdp)+1e-6)) - REWARD_BETA*(delta_co2/(abs(prev_co2)+1e-6))\n",
        "        reward = float(np.nan_to_num(np.clip(reward, -5, 5), nan=0.0))\n",
        "\n",
        "        self.steps += 1\n",
        "        done = self.steps >= self.max_steps\n",
        "        return self._get_obs(), reward, done, False, {}\n",
        "\n",
        "# ------------------------\n",
        "# Train DQN\n",
        "# ------------------------\n",
        "vec_env = DummyVecEnv([lambda: IndustrialEnv()])\n",
        "\n",
        "print(\"\\nTraining DQN...\")\n",
        "model = DQN(\"MlpPolicy\", vec_env, verbose=0, seed=RANDOM_STATE)\n",
        "model.learn(total_timesteps=TIMESTEPS)\n",
        "mean_r, std_r = evaluate_policy(model, vec_env, n_eval_episodes=5)\n",
        "print(f\"\\nDQN → mean_reward={mean_r:.4f} ± {std_r:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWnGbHBQ5tVI",
        "outputId": "916e439f-6c5a-41e1-a309-5e21df930f70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CO2: cement_co2 | GDP: gdp\n",
            "\n",
            "Training DQN...\n",
            "\n",
            "DQN → mean_reward=-0.2607 ± 0.5214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# ------------------------\n",
        "# 1) Load dataset\n",
        "# ------------------------\n",
        "DATA_PATH = \"/content/Data.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"Dataset shape before sampling:\", df.shape)\n",
        "\n",
        "# Sample for speed (10k rows)\n",
        "if len(df) > 10000:\n",
        "    df = df.sample(n=10000, random_state=42)\n",
        "print(\"Dataset shape after sampling:\", df.shape)\n",
        "\n",
        "# ------------------------\n",
        "# 2) Define target\n",
        "# ------------------------\n",
        "possible_target_names = ['target','label','class','y','emission_class','high_emission']\n",
        "target_col = None\n",
        "for t in possible_target_names:\n",
        "    if t in df.columns:\n",
        "        target_col = t\n",
        "        break\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target_col is None:\n",
        "    co2_candidates = [c for c in numeric_cols if \"co2\" in c.lower()]\n",
        "    if co2_candidates:\n",
        "        co2_col = co2_candidates[0]\n",
        "        df['high_emission'] = (df[co2_col] > df[co2_col].median()).astype(int)\n",
        "        target_col = 'high_emission'\n",
        "    else:\n",
        "        raise ValueError(\"No target found and no CO2 column available.\")\n",
        "\n",
        "print(\"Target column:\", target_col)\n",
        "print(df[target_col].value_counts())\n",
        "\n",
        "# ------------------------\n",
        "# 3) Features\n",
        "# ------------------------\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "# ------------------------\n",
        "# 4) Train/test split\n",
        "# ------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5) Preprocessing\n",
        "# ------------------------\n",
        "preprocess = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "X_train = preprocess.fit_transform(X_train)\n",
        "X_test = preprocess.transform(X_test)\n",
        "\n",
        "# ------------------------\n",
        "# 6) Models (fast configs)\n",
        "# ------------------------\n",
        "log_clf = LogisticRegression(max_iter=300)\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "lgbm_clf = LGBMClassifier(n_estimators=50, random_state=42, verbose=-1)\n",
        "\n",
        "# Stacking Ensemble\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_clf),\n",
        "        ('lgbm', lgbm_clf)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=200),\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 7) Train & Evaluate\n",
        "# ------------------------\n",
        "models = {\n",
        "    \"Logistic Regression\": log_clf,\n",
        "    \"Random Forest\": rf_clf,\n",
        "    \"LightGBM\": lgbm_clf,\n",
        "    \"Stacking Ensemble\": stacking_clf\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else y_pred\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_proba)\n",
        "        print(\"ROC AUC:\", auc)\n",
        "        results[name] = auc\n",
        "    except:\n",
        "        results[name] = None\n",
        "\n",
        "# ------------------------\n",
        "# 8) Summary\n",
        "# ------------------------\n",
        "print(\"\\n=== AUC Scores ===\")\n",
        "for k,v in results.items():\n",
        "    print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPzkFpdl6uXU",
        "outputId": "923ce64c-ada8-4a8a-da8a-07e42d5a94ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape before sampling: (43746, 80)\n",
            "Dataset shape after sampling: (10000, 80)\n",
            "Target column: high_emission\n",
            "high_emission\n",
            "0    7314\n",
            "1    2686\n",
            "Name: count, dtype: int64\n",
            "\n",
            "=== Logistic Regression ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9639    0.9923    0.9779      1829\n",
            "           1     0.9773    0.8987    0.9363       671\n",
            "\n",
            "    accuracy                         0.9672      2500\n",
            "   macro avg     0.9706    0.9455    0.9571      2500\n",
            "weighted avg     0.9675    0.9672    0.9668      2500\n",
            "\n",
            "ROC AUC: 0.9945643095711663\n",
            "\n",
            "=== Random Forest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9995    1.0000    0.9997      1829\n",
            "           1     1.0000    0.9985    0.9993       671\n",
            "\n",
            "    accuracy                         0.9996      2500\n",
            "   macro avg     0.9997    0.9993    0.9995      2500\n",
            "weighted avg     0.9996    0.9996    0.9996      2500\n",
            "\n",
            "ROC AUC: 0.9999812590496383\n",
            "\n",
            "=== LightGBM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9989    1.0000    0.9995      1829\n",
            "           1     1.0000    0.9970    0.9985       671\n",
            "\n",
            "    accuracy                         0.9992      2500\n",
            "   macro avg     0.9995    0.9985    0.9990      2500\n",
            "weighted avg     0.9992    0.9992    0.9992      2500\n",
            "\n",
            "ROC AUC: 0.9985072425624908\n",
            "\n",
            "=== Stacking Ensemble ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9989    1.0000    0.9995      1829\n",
            "           1     1.0000    0.9970    0.9985       671\n",
            "\n",
            "    accuracy                         0.9992      2500\n",
            "   macro avg     0.9995    0.9985    0.9990      2500\n",
            "weighted avg     0.9992    0.9992    0.9992      2500\n",
            "\n",
            "ROC AUC: 0.9997808123631605\n",
            "\n",
            "=== AUC Scores ===\n",
            "Logistic Regression: 0.9945643095711663\n",
            "Random Forest: 0.9999812590496383\n",
            "LightGBM: 0.9985072425624908\n",
            "Stacking Ensemble: 0.9997808123631605\n"
          ]
        }
      ]
    }
  ]
}